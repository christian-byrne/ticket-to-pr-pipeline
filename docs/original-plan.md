# Ticket -> PR Pipeline Plan

## Goal

Create a pipeline consisting of agent skills, command, scripts, and mcp integrations that will create a waterfally series of checkpoints/milestones allowing us to go from a ticket to an open PR on ComfyUI_frontend repo.

## Proposed (early draft) Steps in the Pipeline

- Entrypoint: give url to the Notion ticket
- Create research plan which will be carried out by subagents:
    - The research plan is chunked into a subtask for each subagent, each with primary focus on research. The plan therefore is mostly about just taking from a large list of all possible research targets and picking a subset which is appropriate for the given task. I will need help fleshing this out and brainstorming additional sources that we might want to add to the list of possible research targets. We will also want to verify that we are using the best tools and have the best instructions/prompts/context that will serve as a sort of snippet system that we can add to the instructions for the subagents (e.g., "for ComfyUI, use context7 or deepwiki mcp/skill", "for info on how individual nodes work, use the emembedded-docs repo", "for info on UI/UX best practices, refer to these sources (and we beforehand organize and validate that all these snippets work well and do research to flesh them out and expand them")
        - The full Slack thread (using Slack MCP or something) that may or may not be linked in the Notion ticket
        - Other linked or potentially relevant pages on notion. We will have ot setup notion API integration (i have key just need to put somewhere) and also explore top level notion databases and possible useful sources (i can assist if you provide some suggestions and the best way to format the data sources). related tasks on notion for examples
        - commits on main in past 90 days that are related (can setup some good querying snippets and tools, e.g., search for original authors, search for keywords, search for affected files, search for keywords
        - PRs open and closed (using same saerch smenatics as above) -> since we are already querying for commits, in this we might want to focus more on the PR descriptions/titles/and review back and forths to extract context and lessons
        - GitHub Issues (using same search semantics as above)
        - Find the relevant parts of the codebase and the touched/affected files (will have to figure that out first but should often be discernible naturally from the task description and a quick overview of the code)
            - git blame and history on related files
            - documentation in the repo (markdown)
            - established patterns from codebase that are relevant
            - outside research into best practices, libraries, patterns in mature codebases for relevant parts of the code that are similar
    - After each subagent task is created, we spin them all up and let them report back their findings
    - With the findings, we just compile one giant report in markdown temp folder and report it, let human do the next prompt
- The human will then do something like handoff to new session and tell the session to read the report and come up with a plan (therefore the report needs to have included everything we found/disocvered). The prompt for how to create the plan is quite important as we want to make sure to include a bunch of pivot points, decision tree, considerations, risks, pros/cons. Basically, if at any point we made a decision where the next-best decision was almost as good, we want to flag that out and present it as a pivot point. This is a high level plan and doesnt get to code implementation yet. This should also be written to a markdown doc in a tepm directory and then ask for feedback from the human reviewer
- The human reviewer will then go through some iteration cycles on the plan, updating the markdown doc along the way. Then when ready, it will convert it to to implementation details (high level design doc -> more focused implementation/coding tickets basically) and again ask for feedback.
- The human will then do something like create a new session, so everything important for actually coding will now need to be saved in the aforementioned markdown doc
- A skill-mediated step where we ask questions:
    - can this be (is it worth) spliting into multiple PRs either by:
        - slicing vertically into independent distinct PRs (ideal)
        - splitting into stacked PRs with graphite cli or graphite MCP (second best)
        - keeping as a single PR if it makes more sense that way, or everything is highly codependent, or the code change is <200 LoC naturally
    - The human will decide which to do, then the plan will be re-adjusted to reflect that (essentially just change the markdown -- not remove anything -- to have pointed instructions about how to split or not split the code changes into separate PRs)
    - If the graphite stacking or vertical slicing is approved, another skill (need to make) will be invoked to set that up. For stacking, we use graphite as mentioned. For slicing, we use git-worktree-utils which i have already set up on my system and you can referenece the source code repo in cwd for (cloned right now in cwd)
- test driven development pass. for this we already have a skill but might want to make a colocated or parent skill -- the idea is to just assess the current implementation plan and ask if its beneficial to do test-driven-development for it. If iti makes sense (and we generally want to do TDD if possible), then we go look at the testing guidelines and testing strategies available in the repo (maybe we even want a skill to create subagents to research this or maybe precompile it or at least include snippets for how to find the research) -> thikn about how to structure initial tests -> update the implementation plan (update the markdown in place) to reflect how the TDD approach will work -> include that the agents that write the code must verify the tests fail on initial commit of just tests -> pass when the code is stash popped or whatever.
- Once the code is complete, a skill that will invoke all the coding guidelines in our repo (just pnpm scripts like lint, format, knip, stylelint, typecheck, unit tests). We want this skill to involve invoking subagents such that the context is not flooded with the outputs and long waiting time that these commands ential
- Once the above pass is done, another new skill which also involves invoking subagents. This one however will be doing various review passes. The idea is, we invoke a subagent that each does a different type of code review. But instead of the subagent actually making the changes, they just compile all the review comments -> the upper level orchestrator then compiles the complete concatenated list into one giant review -> segments what is worthwhile chaning and what is not based on codebase styles and standards (and also dedupes any repeated or overlapping/conflicting review comments) -> presents an itemized list (and saves it to temp markdown doc) for human to say by number which items to implement, which to skip, and which to implement with slight adjustment, and which need more clarification or research. One of the subagents will be doing coderabbit cli, one will do just normal agent review, and maybe we can add more later or you have other suggestions for linting/review passes for other subagents.
- Lastly, we will have a skill that is optional and human has to choose whether to invoke or not that will use chrome devtools mcp/skill and playwright mcp/plugin/script to verify visually that the change works
- Finally, we will prepare final result for final human review (skill-meidated), this will involve:
    - using tmux skill to open a subwindow and starting up the dev server (use pnpm dev:cloud for cloud features, and pnpm dev for normal features) then in the main window printing the url for the human to go verify
    - also print a QA checklist for the human: everything they should do in the manual review (using the app) to verify the ticket is fully closed, all things in the code reflect what the user is seeing, and so on.
- Once that last pass is approved, a final skill which is for creating the PR, adding appropraite area:* labels, writing a pr description that is brief and concise with no emojis that follows the pr template in the .github of the repo, and so on. Then finally printing the link to the PR. It will then say, "tell me when to check the CI checks" -> the human will wait 30 minutes then say "now" or someting and the agent uses a final skill or instruction to check the CI checks and address/fix anything that the GitHub CI caught like e2e tests or something of that nature. Especially important to check for conflicts with main (maybe we should move that to a skill/check/step above before since we can technically know this before making the PR)?

## Dashboard Tracking and Syncing Status to the Cloud

My initial idea was: PR tracking and dashboard and ticket analysis. Update then pass on, every agent knows how to check status. Notion MCp based

Essentially, I want a way to know what tickets im actively working on, their linked PR, possibly attach/save all the markdown docs we will be making in above steps for each ticket and save them in a known folder -> have a dashboard so i can see what each pipeline session is at and the steps they are at. I would be able to check from anywhere and come back anytime and also see if anything is stuck or needs manual intervention. IDeally we could use notion mcp AT LEAST to add to the ticket in the GitHub PR proeprty/field, assign me, and change status from "Not Started" to "In Progess" to "In Review" to "Done" (but if we cant figure out how to set this up or we are more attached/married to a manual/local dashboard and it will be much faster, that is probably fine -- definitely would like to explore all options).

## Next Steps

- First, i want you to create a diagram in a markdown file in cwd for the pipeline. Then do all the research i suggested in any of the above sections. Then, flesh out the whole plan for what we need to build -- skills to setup, mcps to connect, scripts to make, resarech to pre-compile and save to stable location, a plan for doing the dashboard in notion, everything. We will iterate on this heavily before we actually get to writing and ipmlementing everything. Lastly, we will test the process e2e on a ticket.
